---
title: "Session 4 Notes"
author: "Tim Riffe"
date: "2023-07-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# `for` loops

Motivation:
For the session 3 exercise, we were able to calculate $e^\dagger$ for age 0 only because we don't yet have a good tool to iterate up the ages. The measure can indeed be recalculated at each age, and it's meaningful in each age.

We need a way to iterate (repeat).

First example: let's calculate a cumulative sum with a for loop
```{r}
x <- runif(10)
# a place to store the result:
xc <- x * 0 # we'll over-write this anyway

for (iterator in 1:10){
  xc[iterator] <- sum(x[1:iterator])
}
xc + 2

```

Lots of kinds of parentheses, brackets, and braces.

`()` are for the arguments of functions and function-like things, such as operators.
For the for-loop, the name of the iterator and the values we iterate over are like the arguments.

`[]` square brackets are for indexing

`{}` curly brackets define or delimit bodies of code. We'll use these today a bunch.

In the above example our iterator was used to index a position. But it can be used flexibly. It could be an actual value. It doesn't necessarily need to index a position.

Here's an example of using actual values as the iterator:
```{r}
simpsons <- c("Homer","Marge","Bart","Lisa","Maggie", "Santa's little helper")
for (s in simpsons){
  print(s)
}
```

A nice thing about for-loops is that what you do in one step *can* depend on what you did in previous steps. That is, you can have dependencies in for loops. That's neat.

First, make a vector of length 20
Put the value 1 in the first two elements of it.
Make the iterator go from 3 until 20.
Use the iterator to index position.
The `i`th elements of the result should be the sum of the previous two elements.

```{r}
fib_numbers <- c(1,1,rep(0,18))
for (i in 3:20){
  prev_two         <- (i-2):(i-1)
  fib_numbers[i]   <- sum(fib_numbers[prev_two])
  # fib_numbers[i] <- fib_numbers[prev_two] |> sum()
}
```

Most of the time in practice, an iterator is used to index a position. But there are times where the use is sort of ambiguous, like when you're indexing over single ages, but age is also used on a formula, such that you can use the iterator directly in a formula. Maybe not worth demonstrating, BUT, if you see a loop, the first thing you should do is identify what the iterator is called and how it is being used.

You can also loop over two things at a time (or more). Loops can be inside of loops.

```{r}
A <- matrix(runif(10*20), ncol = 20, nrow = 10)
for (i in 1:10){
  for (j in 1:20){
    A[i,j] <- sum(A[1:i, 1:j])
  }
}
```

This is a sort of prelim for some of the types of data operations that Marie Pier will do. To index a 2-d object (matrix) use the same square brackets, and two index locations, separated by a comma, where the first one refers to rows and the second to columns.

`A[i,j]` means the ith row and jth column of `A`, always.

`A[1:i,1:j]` means the submatrix of A going from the 1st to the ith row and 1st to the jth column.

# Exercise
Use this technique to calculate $e^\dagger$ for all ages.
Tips: 
- make a container called `edag` or similar.
- Index position 1:111
- You need to select out the correct parts of dx and ex for each age step
- `dx` needs to be rescaled each age step in the iteration

```{r}
calc_edag_vec <- function(dx, ex){
  n     <- length(dx)
  # container
  edag  <- rep(0,n)
  
  # i goes up age
  for (i in 1:n){
    # select dx from age i until the top and rescale it
    dxi     <- dx[i:n] / sum(dx[i:n])
    
    # the sum of the product of dxi and ex ages i and higher.
    edag[i] <- sum(dxi * ex[i:n])
  }
  return(edag)
}
```

Let's apply this in a tidy pipeline using `mutate()` (because it should give back a vector the same length as its arguments).

```{r}
library(tidyverse)
source("https://raw.githubusercontent.com/timriffe/BSSD2023Module1/master/my_functions.R")
hmd <- read_csv("https://github.com/timriffe/BSSD2023Module1/raw/master/Data/hmd.csv.gz", show_col_types = FALSE)
hmd_completed <-
  hmd |> 
  group_by(country, sex, year) |> 
  group_modify(~ my_lifetable_tibble(data_chunk = .x)) |> 
  mutate(edag = calc_edag_vec(dx, ex)) |> 
  ungroup()
```

Plot the ex by edag in scatterplots by sex, for selected ages e.g. 0,15,45,65

```{r, warning = FALSE}
hmd_completed |> 
  filter(age %in% c(0,15,45,65)) |> 
  ggplot(mapping = aes(x = ex,
                       y = edag,
                       color = sex)) +
  geom_point(alpha = .1) +
  facet_grid(rows = vars(age),  
             cols = vars(sex))
```

# conditional programming

You can toggle whether or not a body of code gets executed using an `if` statement.
For this, you need to define a condition.

```{r}
x <- rnorm(10)
if (mean(x)){
  print("You win!")
}
```

In the above example, `x` contains random draws from a standard normal, and their actual mean could be greater or less than zero. In the `if()` statement we specify only to execute the code in the body if the mean is greater than 0.

You can set the condition to TRUE or FALSE by hard-coding it too. In that case the code will always or never be executed. That sounds weird, but in practice can be useful. Say you have some heavy calculations or simulations or whatever sitting in a script, and you don't want to ever execute them without your manual intervention. Then you can wrap that code inside of an if statement set to to FALSE. Then the only way to execute it is via your manual intervention.

I showed some examples of `if` being used in functions to toggle behavior. For example, we might imagine a different closeout method for our lifetable, so I added a `close_method` argument to it, with a default value set to pick our current behavior (constant hazard). But it could be modified in the future if we had other methods for that.

We also saw how `if` was used in `LifeIneq` functions to let the users decide whether to perform data checks before calculating, or to adapt code to different choice of central tendency (mean, median, mode, etc)- all done with `if`.

```{r}
x <- rnorm(10)
if (mean(x) > 0){
  print("You win!")
} else {
  print("You lose!")
}
```

These constructions can also nest, but please do try to avoid this:
```{r}
x <- rnorm(10)
the_mean <- mean(x)
print(mean(x))
if (the_mean > 0){
  print("You win!")
} else {
  if (the_mean <= 0 & the_mean > -.1){
    print("I don't know whether you win or lose")
  } else {
    print("You lose!")
  }
}
```

Really, only build code like that if it's totally necessary. You cna probably see how legibility reduces as complexity increases with nested if statements.

# If and for together

Newton's method for finding a square root:
```{r}
x     <- 123 # the number we want the sqrt of
guess <- 2   # <- a terrible guess
newton_sqrt <- function(x, guess = 5, maxit = 100, tol = 10^-12){
  for(i in 1:maxit){
    # step 1: calculate the "other part" of the supposed
    quotient     <- x / guess
    # average w previous guess
    average      <- (quotient + guess)/2
    # store the old guess so that we can check if we've converged
    oldguess     <- guess
    # the new guess comes from current average
    guess        <- average
    
    # this is for checking convergence
    abs_change   <- abs(guess-oldguess)
    if (abs_change <= tol){
      # if the above evaulates to TRUE, then
      # the loop stops, using break
      break
    } 
  }
  return(guess)
}
guess
sqrt(123)
 # Re a question on making the random guess come from a dist
newton_sqrt(123, guess = rpois(1, 10))
```

New concepts:
`break`: you can make a loop stop running before it gets to the end of its iterator by triggering `break` with an `if` statement.

We then wrapped the algorithm in a function to make our own square root calculator. That was an afterthought, for the sake of review. We decided to foresee which elements of the algorithm a user might want to change, and to turn them into arguments: `maxit` and `tol`, which were previously hard coded to what their current default values are.

# Again, with a different algorithm:

Here's let's make a function that calculates the Collatz number of a positive integer. The Collatz conjecture states that any positive integer can be reduced to one by following this algorithm:
1.    If the integer is 1 then stop
2.    If the integer is even, then divide by 2
3.    If the integer is odd, multiply by 3 and add 1.
The number of steps it takes to get to 1 is the Collatz number of the integer. This is as far as I know an unproven conjecture. This function does it, let's take a tour and annotate as we go.
```{r}

Collatz <- function(number, maxit = 1e5){
	number  <- as.integer(number)
	Cnumber <- 0 
	for (fake.iterator in 1:maxit){ 
	  if (number == 1){
	    break
	  } else {
			Cnumber <- Cnumber + 1    
			if (number %% 2 == 0){
				number <- number / 2
			} else {
				number <- number * 3 + 1
			}
	  }
	}
	return(Cnumber)
}
```
The above function implements the calculation steps for a Collatz number. `if` is used to trigger a break in the loop if we're done. It's also used to trigger which arithmetic transformation we need to apply, depending on which we have a positive or negative integer at this step.
`%%` gives us a remainder after division. It's 0 if the number divides cleanly. We often use modulo in demography to group ages. Like so:

```{r}
x <- rpois(20, lambda = 10)
age <- 0:19
tibble(x=x, age = age) |> 
  mutate(age5 = age - age %% 5) |> 
  group_by(age5) |> 
  summarize(x = sum(x))
```
Modulo (`%%`) is being used here to determine which 5-year age group a given single age belongs to. Then we can use `dplyr` tricks to perform the aggregation. We want to aggregate in groups, where groups are the new age categories, ergo `group_by(age5)`. The aggregation action is done with summarize, in this case overwriting `x`. You might want to use this trick if ever combining data sources delivered in different age groups. The alternative is to somehow bring everything to single ages and then merge.

```{r}
how_high              <- 10000
my_integers           <- 1:how_high
their_collatz_numbers <- rep(0,how_high)
for (i in 1:how_high){
  their_collatz_numbers[i] <- Collatz(my_integers[i])
}

tibble(i = my_integers,
       Collatz = their_collatz_numbers) |> 
  ggplot(aes(x = i, y = Collatz)) +
  geom_point()
```

# Exercise

Let's implement Ansely Coale's 1955 proposal to optimize the intrinsic growth rate `r`.


```{r}
# Loads two objects: Lx and fxf
source("https://gist.githubusercontent.com/timriffe/4d21788219b1c6ec0dc0acec9cd7f2fa/raw/a4e039d23d536e84edc9547a34439cf4a3b67f84/coale.R")

R0 = sum(Lx * fxf)
G = 29
```

$$R_0 = e^{r*G}$$
$$log(R_0) = r* G$$
$$log(R_0) / G = r$$

1. $L(x)$
2. $f(x)^f$ ASFR for girl births
3. $x$ a vector of ages.

Steps to follow:

1. calculate $$R(0) = \sum L(x)f(x)^f$$
2. assume a parameter $G = 29$
3. calculate a guess at $r$, the first of several, call it $r^i$, the $i^{th}$ guess: $$r^i = \frac{log(R(0))}{G}$$
4. Now in a for-loop we update $r^i$ in two steps. 
   i) calculate a residual $$\delta = \left[\sum e^{(-r^i x)}f(x)^fL(x)\right] - 1$$ 
   ii) update $r^i$ using $$r^{i+1}= r^i + \frac{\delta}{G-\frac{\delta}{r^i}}$$
5. Repeat step 4 until $\delta$ is teeny tiny, i.e. until $$1 = \sum e^{(-r^i x)}f(x)^fL(x)$$
Usually this takes less than 10 iterations, but you can let it go for more than that.

When you get the loop written, wrap it in a function whose arguments are $f(x)^f$, $L(x)$, and $x$, and which returns $r$.


```{r}
coale_r <- function(fxf, Lx, G = 29, maxit = 50, tol = 10^-12){
  R0 <- sum(fxf * Lx)
  ri <- log(R0) / G
  x  <- 0:(length(fxf)-1)
  for (i in 1:maxit){
    deltai <- sum(exp(-ri * x) * fxf * Lx) - 1
    ri     <- ri + (deltai / (G - (deltai / ri)))
    
    # this is just like delti, isn't it?
    resid <- abs(sum(exp(-ri * x) * fxf * Lx) - 1)
    if (resid < tol){
      break
    }
  }
  return(ri)
}

r <- coale_r(fxf, Lx, G = 29)
```

# Review of recent concepts

Functions, they are handy, and sometimes you can get away with writing one *after* the hard work is done. i.e. after you figure out how to solve a problem with code, then much of the time, it doesn't cost too much extra work to turn it into a function. That might save you or others time in the future.

`for` loops for repetitive code execution i.e. iteration. We've seen iterators being used to index positions (in your data inputs, or in the output object that you're populating in the loop). We've also seen, in an earlier rendition of the Collatz function, the iterator being used for its actual value. Usually we just use them for indexing though. Or not at all.

`if` triggers whether or not a block of code (inside `{}`) gets executed or not.

But `if` all by itself, or `if` together with `else` would be a very clunky approach to doing something like recoding a variable. Which is a common problem to solve.

# `if_else()`
```{r}

x <- 1:10
if_else(x %% 2 == 0,  # needs to be TRUE or FALSE for each value of x
        x / 2, # if it's TRUE we do this
        x * 3 + 1) # FALSE then do this
(x %% 2 == 0)
```

How can we recode with this construct?
```{r}
A <- tibble(variable = letters[1:5])
A |> 
  mutate(new_codes = if_else(variable %in% c("a","b"),
                             "Group 1",
                             "Group 2"))

A |> 
  mutate(new_codes = if_else(variable %in% c("a","b"),
                            "Low",
                            if_else(variable %in% c("d","e"),
                                    "High",
                                    "Middle")))
```

# `case_when()` is the tool for you

When you have 3 or more result categories, then use `case_when()`. Comma-separate the cases, where each case follows the pattern: `logical condition ~ new code`. You either need to account for all cases (might be laborious) or include a catch-all at the end, using `TRUE`.
```{r}
A |> 
  mutate(new_codes = case_when(variable %in% c("a","b") ~ "Low",
                               variable == "c" ~ "Middle",
                               TRUE ~ "High"))
```

discretization: how to do this for quantitative variables. In this case, just decide whether the output is numeric or character (or something else)

```{r}
A |> 
  mutate(var2 = runif(5),
         var2_binned =
           case_when(var2 < .4 ~ "Low",
                     var2 > .6 ~ "High",
                     TRUE ~ "Middle"),
         var2_binnedq =
           case_when(var2 < .4 ~ 0,
                     var2 > .6 ~ .6,
                     TRUE ~ .4))
```

There are helper functions for binning in predefined intervals, or within intervals of a specifed width:
```{r}
cut_interval(1:100, 10)
table(cut_interval(1:100, 11))
table(cut_width(runif(1000), 0.1))
A |> 
  mutate(var2 = runif(5),
         var3 = cut_width(var2, .2))
```

For truncation, simple use `filter()`, but to aggregate to an open age group, recode age inside `mutate()`, then group by age and aggregate using `summarize()`.

For the above case I imagined the situation where you have register microdata (big) with a quantitative variable that you want to bin. Then 1. use `cut_interval()` or `cut_width()` to recode that variable and 2. `group_by() |> summarize()` to aggregate it.

# `*_join()` for more complicated cases

Use the `left_join()` approach rather than the `case_when()` approach whenever you have tons of codes to account for. Do it this way because then your lookup table is (i) outside the code (reduce clutter), (ii) can be independently maintained, (iii) can be shared. This definitely makes sense for geographic areas, and for ICD codes, but maybe also other things beyond my experience.
```{r}
gapreminder <- read_csv("https://raw.githubusercontent.com/timriffe/BSSD2023Module1/master/Data/gapreminder.csv", show_col_types = FALSE)

lookup <- read_csv("https://raw.githubusercontent.com/timriffe/BSSD2023Module1/master/Data/iso3continent.csv",show_col_types = FALSE)

gapreminder |> 
  left_join(lookup, by = "iso3")
```

# Review

1. You can do a lot if your data is already tidy. We did that with `ggplot()` graphics.
2. `ggplot()` composes a plot by specifying a mapping using `mapping = aes()`. That should include some coordinates (x, or y, or both), and potentially other things like `color`, `size`, etc. `groups` is also a mapping.
3. then you need a `geom_*()` or more.
4. `scale_*()` to log an axis or do some other similar transformation

I encourage you to get really comfortable with this plotting package, and to always visualize as you do your coding. This is to make sure things are behaving as expected. The vast majority of plots are diagnostic plots meant to be seen by you only. Don't invest time in those, just make them quick and cheap.

But, you can only map a variable if it's represented as a column in your data. Meaning your data has to be tidy. What if it isn't?

Then you need to do data wrangling. Here's a similar workshop repository on that: <https://github.com/timriffe/EDSD2022data>
Also see the data wrangling cheat sheet, and tidy data cheat sheet from R studio.

We saw functions for `read_*()`ing in data from excel, csv. Reading from spreadsheets takes a bit of extra work to figure out the spreadsheet. csv is the most shareable data format.

We saw how to `filter()` rows and `select()` / `rename()` columns.
We saw how to `mutate()`, `summarize()`, and how to do so within groups using `group_by()`. Never forget to `ungroup()` when you're done.

`pivot_longer()` is used to stack variables
`pivot_wider()` spreads out a variable over columns

You can rip off a column using `pull()`, which we did in order to get test variables for lifetables and similar.

In the handouts you'll see `separate()` being used to split a column.

We also saw `*_join()` for merging data after categories for the key have already been harmonized.

That's all for data wrangling (Tuesday)

Wednesday we saw function programming on the example of the life table. 
We set it up for modular functions. The means, we made a function for each column transformation. We then combined our functions in what we call wrapper functions, in order to produce the whole life table, or to make a second version fo the lifetable function that can click into a tidy pipeline (data.frame in, data.frame out, using `group_modify()`).

We then applied our lifetable function to a big dataset using `group_by()`.

I argued that creating functions allows for easier sharing of methods, and that it's also better for you because then you only need to maintain the code in one place. I showed a primitive way of reading in functions using `source()` from github. The next best thing is to make a package, which is a different class. I also or described how people use `source()` to set up their reproducibility packs.

Then we talked about more specific programming tools, like iteration `for` and conditional execution, with `if`. We even combinded these tools in three different functions today.

Finally we saw how cool `case_when()` is for recoding, and to use `left_join()` for larger cases.

You got a course on reproducibility for free because we did everything inside of R projects of RStudio, which are self-contained (hermetically sealed) projects, ergo portable, and we forced the code to be clean by using `Rmarkdown`, and also taught *literate* programming. That means text and code together. That's a great tool both for research, but also report generation.

Remember, you can decide whether the code should be shown or hidden when sharing output generated by markdown. So cater to your audience.

I also mentioned that you might want to check out the code produced by researchers that you follow.

















































```{r}

```






























